{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pok_env_gym_RLLib import PokemonEnv\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "\n",
    "with open(\"env_config.yaml\", 'r') as file:\n",
    "    env_config = yaml.safe_load(file)\n",
    "\n",
    "def env_creator(env_config):\n",
    "    return PokemonEnv(config=env_config, render_mode='human')  # return an env instance\n",
    "\n",
    "register_env(\"my_env\", env_creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 255, (100, 100), uint8)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "{'current_hp': 0.0, 'levels': 0, 'badges': 0, 'pokedex_count': 0}\n"
     ]
    }
   ],
   "source": [
    "env = PokemonEnv(config=env_config)\n",
    "print(env.observation_space)\n",
    "print(env._get_obs())\n",
    "print(env._get_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 11:00:51,442\tWARNING util.py:62 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'custom_metrics': {},\n",
       " 'episode_media': {},\n",
       " 'info': {'learner': {},\n",
       "  'num_env_steps_sampled': 1000,\n",
       "  'num_env_steps_trained': 0,\n",
       "  'num_agent_steps_sampled': 1000,\n",
       "  'num_agent_steps_trained': 0},\n",
       " 'sampler_results': {'episode_reward_max': nan,\n",
       "  'episode_reward_min': nan,\n",
       "  'episode_reward_mean': nan,\n",
       "  'episode_len_mean': nan,\n",
       "  'episode_media': {},\n",
       "  'episodes_this_iter': 0,\n",
       "  'policy_reward_min': {},\n",
       "  'policy_reward_max': {},\n",
       "  'policy_reward_mean': {},\n",
       "  'custom_metrics': {},\n",
       "  'hist_stats': {'episode_reward': [], 'episode_lengths': []},\n",
       "  'sampler_perf': {},\n",
       "  'num_faulty_episodes': 0,\n",
       "  'connector_metrics': {}},\n",
       " 'episode_reward_max': nan,\n",
       " 'episode_reward_min': nan,\n",
       " 'episode_reward_mean': nan,\n",
       " 'episode_len_mean': nan,\n",
       " 'episodes_this_iter': 0,\n",
       " 'policy_reward_min': {},\n",
       " 'policy_reward_max': {},\n",
       " 'policy_reward_mean': {},\n",
       " 'hist_stats': {'episode_reward': [], 'episode_lengths': []},\n",
       " 'sampler_perf': {},\n",
       " 'num_faulty_episodes': 0,\n",
       " 'connector_metrics': {},\n",
       " 'num_healthy_workers': 2,\n",
       " 'num_in_flight_async_reqs': 0,\n",
       " 'num_remote_worker_restarts': 0,\n",
       " 'num_agent_steps_sampled': 1000,\n",
       " 'num_agent_steps_trained': 0,\n",
       " 'num_env_steps_sampled': 1000,\n",
       " 'num_env_steps_trained': 0,\n",
       " 'num_env_steps_sampled_this_iter': 1000,\n",
       " 'num_env_steps_trained_this_iter': 0,\n",
       " 'num_env_steps_sampled_throughput_per_sec': 14.789641310266964,\n",
       " 'num_env_steps_trained_throughput_per_sec': 0.0,\n",
       " 'timesteps_total': 1000,\n",
       " 'num_steps_trained_this_iter': 0,\n",
       " 'agent_timesteps_total': 1000,\n",
       " 'timers': {'training_iteration_time_ms': 135.64, 'sample_time_ms': 135.355},\n",
       " 'counters': {'num_env_steps_sampled': 1000,\n",
       "  'num_env_steps_trained': 0,\n",
       "  'num_agent_steps_sampled': 1000,\n",
       "  'num_agent_steps_trained': 0},\n",
       " 'done': False,\n",
       " 'episodes_total': 0,\n",
       " 'training_iteration': 1,\n",
       " 'trial_id': 'default',\n",
       " 'date': '2024-03-07_11-01-59',\n",
       " 'timestamp': 1709805719,\n",
       " 'time_this_iter_s': 67.61592864990234,\n",
       " 'time_total_s': 67.61592864990234,\n",
       " 'pid': 67409,\n",
       " 'hostname': 'Joeys-MacBook-Pro.local',\n",
       " 'node_ip': '127.0.0.1',\n",
       " 'config': {'extra_python_environs_for_driver': {},\n",
       "  'extra_python_environs_for_worker': {},\n",
       "  'num_gpus': 0,\n",
       "  'num_cpus_per_worker': 1,\n",
       "  'num_gpus_per_worker': 0,\n",
       "  '_fake_gpus': False,\n",
       "  'num_learner_workers': 0,\n",
       "  'num_gpus_per_learner_worker': 0,\n",
       "  'num_cpus_per_learner_worker': 1,\n",
       "  'local_gpu_idx': 0,\n",
       "  'custom_resources_per_worker': {},\n",
       "  'placement_strategy': 'PACK',\n",
       "  'eager_tracing': True,\n",
       "  'eager_max_retraces': 20,\n",
       "  'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'log_device_placement': False,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'allow_soft_placement': True},\n",
       "  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "   'inter_op_parallelism_threads': 8},\n",
       "  'torch_compile_learner': False,\n",
       "  'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>,\n",
       "  'torch_compile_learner_dynamo_backend': 'aot_eager',\n",
       "  'torch_compile_learner_dynamo_mode': None,\n",
       "  'torch_compile_worker': False,\n",
       "  'torch_compile_worker_dynamo_backend': 'aot_eager',\n",
       "  'torch_compile_worker_dynamo_mode': None,\n",
       "  'env': 'my_env',\n",
       "  'env_config': {'rom_path': 'jeu/PokemonRed.gb',\n",
       "   'exp_memory_size': 20000,\n",
       "   'ep_length': 16384,\n",
       "   'init_state': 'jeu/init_state_pokeball.state',\n",
       "   'nb_action': 6,\n",
       "   'emulation_speed': 0,\n",
       "   'start_level': 8,\n",
       "   'render_reward': False,\n",
       "   'im_dim': [100, 100],\n",
       "   'sim_frame_dist': 5500,\n",
       "   'render_view': True,\n",
       "   'action_freq': 24,\n",
       "   'video_path': '',\n",
       "   'save_video': True},\n",
       "  'observation_space': None,\n",
       "  'action_space': None,\n",
       "  'env_task_fn': None,\n",
       "  'render_env': False,\n",
       "  'clip_rewards': None,\n",
       "  'normalize_actions': True,\n",
       "  'clip_actions': False,\n",
       "  'disable_env_checking': False,\n",
       "  'auto_wrap_old_gym_envs': True,\n",
       "  'action_mask_key': 'action_mask',\n",
       "  '_is_atari': None,\n",
       "  'env_runner_cls': None,\n",
       "  'num_envs_per_worker': 1,\n",
       "  'enable_connectors': True,\n",
       "  'rollout_fragment_length': 'auto',\n",
       "  'batch_mode': 'truncate_episodes',\n",
       "  'validate_workers_after_construction': True,\n",
       "  'compress_observations': False,\n",
       "  'sampler_perf_stats_ema_coef': None,\n",
       "  'sample_async': -1,\n",
       "  'remote_worker_envs': False,\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'enable_tf1_exec_eagerly': False,\n",
       "  'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'update_worker_filter_stats': True,\n",
       "  'use_worker_filter_stats': True,\n",
       "  'gamma': 0.99,\n",
       "  'lr': 0.0005,\n",
       "  'grad_clip': 40.0,\n",
       "  'grad_clip_by': 'global_norm',\n",
       "  'train_batch_size': 32,\n",
       "  'model': {'_disable_preprocessor_api': False,\n",
       "   '_disable_action_flattening': False,\n",
       "   'fcnet_hiddens': [256, 256],\n",
       "   'fcnet_activation': 'tanh',\n",
       "   'conv_filters': None,\n",
       "   'conv_activation': 'relu',\n",
       "   'post_fcnet_hiddens': [],\n",
       "   'post_fcnet_activation': 'relu',\n",
       "   'free_log_std': False,\n",
       "   'no_final_linear': False,\n",
       "   'vf_share_layers': True,\n",
       "   'use_lstm': False,\n",
       "   'max_seq_len': 20,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action': False,\n",
       "   'lstm_use_prev_reward': False,\n",
       "   '_time_major': False,\n",
       "   'use_attention': False,\n",
       "   'attention_num_transformer_units': 1,\n",
       "   'attention_dim': 64,\n",
       "   'attention_num_heads': 1,\n",
       "   'attention_head_dim': 32,\n",
       "   'attention_memory_inference': 50,\n",
       "   'attention_memory_training': 50,\n",
       "   'attention_position_wise_mlp_dim': 32,\n",
       "   'attention_init_gru_gate_bias': 2.0,\n",
       "   'attention_use_n_prev_actions': 0,\n",
       "   'attention_use_n_prev_rewards': 0,\n",
       "   'framestack': True,\n",
       "   'dim': 84,\n",
       "   'grayscale': False,\n",
       "   'zero_mean': True,\n",
       "   'custom_model': None,\n",
       "   'custom_model_config': {},\n",
       "   'custom_action_dist': None,\n",
       "   'custom_preprocessor': None,\n",
       "   'encoder_latent_dim': None,\n",
       "   'always_check_shapes': False,\n",
       "   'lstm_use_prev_action_reward': -1,\n",
       "   '_use_default_native_models': -1},\n",
       "  'optimizer': {},\n",
       "  'max_requests_in_flight_per_sampler_worker': 2,\n",
       "  '_learner_class': None,\n",
       "  'explore': True,\n",
       "  'exploration_config': {'type': 'EpsilonGreedy',\n",
       "   'initial_epsilon': 1.0,\n",
       "   'final_epsilon': 0.02,\n",
       "   'epsilon_timesteps': 10000},\n",
       "  'algorithm_config_overrides_per_module': {},\n",
       "  'policy_map_capacity': 100,\n",
       "  'policy_mapping_fn': <function ray.rllib.algorithms.algorithm_config.AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN(aid, episode, worker, **kwargs)>,\n",
       "  'policies_to_train': None,\n",
       "  'policy_states_are_swappable': False,\n",
       "  'observation_fn': None,\n",
       "  'count_steps_by': 'env_steps',\n",
       "  'input_config': {},\n",
       "  'actions_in_input_normalized': False,\n",
       "  'postprocess_inputs': False,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'output': None,\n",
       "  'output_config': {},\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'offline_sampling': False,\n",
       "  'evaluation_interval': None,\n",
       "  'evaluation_duration': 10,\n",
       "  'evaluation_duration_unit': 'episodes',\n",
       "  'evaluation_sample_timeout_s': 180.0,\n",
       "  'evaluation_parallel_to_training': False,\n",
       "  'evaluation_config': {'explore': False},\n",
       "  'off_policy_estimation_methods': {},\n",
       "  'ope_split_batch_by_episode': True,\n",
       "  'evaluation_num_workers': 0,\n",
       "  'always_attach_evaluation_results': False,\n",
       "  'enable_async_evaluation': False,\n",
       "  'in_evaluation': False,\n",
       "  'sync_filters_on_rollout_workers_timeout_s': 60.0,\n",
       "  'keep_per_episode_custom_metrics': False,\n",
       "  'metrics_episode_collection_timeout_s': 60.0,\n",
       "  'metrics_num_episodes_for_smoothing': 100,\n",
       "  'min_time_s_per_iteration': None,\n",
       "  'min_train_timesteps_per_iteration': 0,\n",
       "  'min_sample_timesteps_per_iteration': 1000,\n",
       "  'export_native_model_files': False,\n",
       "  'checkpoint_trainable_policies_only': False,\n",
       "  'logger_creator': None,\n",
       "  'logger_config': None,\n",
       "  'log_level': 'WARN',\n",
       "  'log_sys_usage': True,\n",
       "  'fake_sampler': False,\n",
       "  'seed': None,\n",
       "  'ignore_worker_failures': False,\n",
       "  'recreate_failed_workers': False,\n",
       "  'max_num_worker_restarts': 1000,\n",
       "  'delay_between_worker_restarts_s': 60.0,\n",
       "  'restart_failed_sub_environments': False,\n",
       "  'num_consecutive_worker_failures_tolerance': 100,\n",
       "  'worker_health_probe_timeout_s': 60,\n",
       "  'worker_restore_timeout_s': 1800,\n",
       "  '_rl_module_spec': None,\n",
       "  '_AlgorithmConfig__prior_exploration_config': None,\n",
       "  '_enable_new_api_stack': False,\n",
       "  '_tf_policy_handles_more_than_one_loss': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  '_disable_action_flattening': False,\n",
       "  '_disable_execution_plan_api': True,\n",
       "  '_disable_initialize_loss_from_dummy_batch': False,\n",
       "  'simple_optimizer': False,\n",
       "  'policy_map_cache': -1,\n",
       "  'worker_cls': -1,\n",
       "  'synchronize_filters': -1,\n",
       "  'replay_sequence_length': None,\n",
       "  'target_network_update_freq': 4,\n",
       "  'replay_buffer_config': {'type': ray.rllib.utils.replay_buffers.multi_agent_prioritized_replay_buffer.MultiAgentPrioritizedReplayBuffer,\n",
       "   'prioritized_replay': -1,\n",
       "   'capacity': 60000,\n",
       "   'prioritized_replay_alpha': 0.5,\n",
       "   'prioritized_replay_beta': 0.5,\n",
       "   'prioritized_replay_eps': 3e-06,\n",
       "   'replay_sequence_length': 1,\n",
       "   'worker_side_prioritization': False},\n",
       "  'num_steps_sampled_before_learning_starts': 1000,\n",
       "  'store_buffer_in_checkpoints': False,\n",
       "  'lr_schedule': None,\n",
       "  'adam_epsilon': 1e-08,\n",
       "  'tau': 1.0,\n",
       "  'num_atoms': 1,\n",
       "  'v_min': -10.0,\n",
       "  'v_max': 10.0,\n",
       "  'noisy': False,\n",
       "  'sigma0': 0.5,\n",
       "  'dueling': False,\n",
       "  'hiddens': [256],\n",
       "  'double_q': False,\n",
       "  'n_step': 1,\n",
       "  'before_learn_on_batch': None,\n",
       "  'training_intensity': None,\n",
       "  'td_error_loss_fn': 'huber',\n",
       "  'categorical_distribution_temperature': 1.0,\n",
       "  'input': 'sampler',\n",
       "  'policies': {'default_policy': (None, None, None, None)},\n",
       "  'callbacks': ray.rllib.algorithms.callbacks.DefaultCallbacks,\n",
       "  'create_env_on_driver': False,\n",
       "  'custom_eval_function': None,\n",
       "  'framework': 'torch',\n",
       "  'num_cpus_for_driver': 1,\n",
       "  'num_workers': 2},\n",
       " 'time_since_restore': 67.61592864990234,\n",
       " 'iterations_since_restore': 1,\n",
       " 'perf': {'cpu_util_percent': 10.074226804123711,\n",
       "  'ram_util_percent': 60.07731958762885}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.rllib.algorithms.dqn.dqn import DQNConfig, DQN\n",
    "config = DQNConfig()\n",
    "\n",
    "replay_config = {\n",
    "        \"type\": \"MultiAgentPrioritizedReplayBuffer\",\n",
    "        \"capacity\": 60000,\n",
    "        \"prioritized_replay_alpha\": 0.5,\n",
    "        \"prioritized_replay_beta\": 0.5,\n",
    "        \"prioritized_replay_eps\": 3e-6,\n",
    "    }\n",
    "\n",
    "config = config.training(replay_buffer_config=replay_config,\n",
    "                         target_network_update_freq=4,\n",
    "                         n_step=1,\n",
    "                         num_atoms=1,\n",
    "                         double_q=False,\n",
    "                         dueling=False)\n",
    "config = config.resources(num_gpus=0)\n",
    "config = config.rollouts(num_rollout_workers=2)\n",
    "config = config.environment(\"my_env\", env_config=env_config)\n",
    "algo = DQN(config=config)\n",
    "algo.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
